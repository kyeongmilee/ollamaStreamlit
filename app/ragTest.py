#import streamlit as st
from dotenv import load_dotenv
from PyPDF2 import PdfReader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings
from langchain.vectorstores import FAISS
from langchain.vectorstores import Chroma
from langchain.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.llms import HuggingFaceHub
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_community.llms import HuggingFacePipeline
from langchain.embeddings import HuggingFaceEmbeddings
from langchain_community.document_loaders import TextLoader
# Load model directly
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from langchain.retrievers import BM25Retriever, EnsembleRetriever # pip install rank_bm25
from langchain import hub
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain.document_loaders import PyPDFLoader
from langchain.indexes import VectorstoreIndexCreator
from langchain.chains import RetrievalQA
import torch
import os
from langchain.retrievers.multi_query import MultiQueryRetriever
import time
from langchain.storage import InMemoryByteStore
from langchain.storage import LocalFileStore
from langchain.embeddings import CacheBackedEmbeddings
import math
from langchain.cache import SQLiteCache
from langchain.globals import set_llm_cache
import time


def get_pdf_text(pdf_docs):
    text = ""
    for pdf in pdf_docs:
        pdf_reader = PdfReader(pdf)
        for page in pdf_reader.pages:
            text += page.extract_text()
    return text


def get_text_chunks(text):
    text_splitter = CharacterTextSplitter(
        separator="\n",
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
    )
    chunks = text_splitter.split_text(text)
    return chunks


def get_vectorstore(text_chunks):
    # embeddings = OpenAIEmbeddings()
    embeddings = HuggingFaceInstructEmbeddings(model_name="hkunlp/instructor-xl")
    vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)
    return vectorstore


def get_conversation_chain(vectorstore):
    # llm = ChatOpenAI()
    # llm = HuggingFaceHub(repo_id="google/flan-t5-xxl", model_kwargs={"temperature":0.5, "max_length":512})
    llm = HuggingFacePipeline.from_model_id(
        model_id="megastudy/M-SOLAR-10.7B-v1.3", 
        device=0,               # -1: CPU(default), GPUì¡´ì¬í•˜ëŠ” ê²½ìš°ëŠ” 0ë²ˆì´ìƒ(CUDA Device #ë²ˆí˜¸)
        task="text-generation", # í…ìŠ¤íŠ¸ ìƒì„±
        model_kwargs={"temperature": 0.1, # ìƒì„±ëœ í…ìŠ¤íŠ¸ì˜ ë‹¤ì–‘ì„± ì¡°ì •
                    "max_length": 128},# ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜   
    )

    memory = ConversationBufferMemory(
        memory_key='chat_history', return_messages=True)
    conversation_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vectorstore.as_retriever(),
        memory=memory
    )
    return conversation_chain


def handle_userinput(user_question):
    response = st.session_state.conversation({'question': user_question})
    st.session_state.chat_history = response['chat_history']

    for i, message in enumerate(st.session_state.chat_history):
        if i % 2 == 0:
            st.write(user_template.replace(
                "{{MSG}}", message.content), unsafe_allow_html=True)
        else:
            st.write(bot_template.replace(
                "{{MSG}}", message.content), unsafe_allow_html=True)

model_id = "meta-llama/Meta-Llama-3-8B-Instruct"
#"cocoirun/AIFT-42dot-PLM-1.3B-ao-instruct-all-v0.4-ff-e1"
#"sue3489/test0_kullm-polyglot-5.8b-v2-koalpaca-v1.1b"#ë„ˆë¬´ ëŠë¦¼
#"mu0gum/AIFT-42dot_LLM-PLM-1.3B-ao-instruct-all-v1.3"#ë‹µë³€ì´ ë³„ë¡œì•¼
#"sue3489/test0_kullm-polyglot-5.8b-v2-koalpaca-v1.1b" #ì¶”ë¡ ì‹œ í† í°ê¶Œí•œ ë¬¸ì œ ë°œìƒ
#"Xwin-LM/Xwin-LM-7B-V0.2" #í† í¬ë‚˜ì´ì € ì—ëŸ¬
#"julleong/illuni-llama-2-ko-7b-test" #1ë¶„ ì´ìƒ ì†Œìš”
#"AIFT/AIFT-instruct-SFT-1.3B-refine-v3"#24ì´ˆ ì†Œìš” ë‹µë³€ ê¸¸ì–´ 
#"mu0gum/AIFT-42dot_LLM-PLM-1.3B-ao-instruct-all-v1.3" #í˜„ì¬ê¹Œì§€ ì ¤ ë‚˜ì´ìŠ¤
#"jungyuko/DAVinCI-42dot_LLM-PLM-1.3B-v1.5.3"# 23ì´ˆ ì—‰ëš±ë‹µ
#"Changgil/K2S3-SOLAR-11b-v2.0" #ì‘ë‹µì´ ì•ˆì˜´
#"cocoirun/AIFT-42dot-PLM-1.3B-ao-instruct-all-v0.4-ff-e1"#ë² ë¦¬êµ¿
#"Changgil/K2S3-Llama2-13b-v1.0" #ë©”ëª¨ë¦¬ ì•„ì›ƒ 
#"momo/polyglot-ko-12.8b-Chat-QLoRA-Merge" # ë©”ëª¨ë¦¬ ì•„ì›ƒ 47.53
#"gwonny/llama-2-koen-13b-QLoRA-NEFTune-kolon-v0.1"ë©”ëª¨ë¦¬ 47.53.#"mu0gum/AIFT-polyglot-ko-1.3b-ao-instruct-v0.91"#"nlpai-lab/kullm-polyglot-5.8b-v2"#"beomi/open-llama-2-ko-7b"#"Changgil/K2S3-Llama2-13b-v1.0"#"KRAFTON/KORani-v2-13B"#"beomi/open-llama-2-ko-7b"#"yanolja/KoSOLAR-10.7B-v0.2"#"mu0gum/AIFT-polyglot-ko-1.3b-ao-instruct-v0.91"
model_path = "./"+model_id
tokenizer_path = "./"+model_id+"-tokenizer"

def settingModel() :
    global model
    global tokenizer
    global device

    ##ìºì‰¬ ë¹„ìš°ê¸°
    torch.cuda.empty_cache()

    ## ëª¨ë¸ êµ¬ë™ í™˜ê²½ ì„¤ì •
    device = (
        torch.device("cuda:0") if torch.cuda.is_available() else torch.device("cpu")
    )

    
    # ëª¨ë¸ì´ ë¡œì»¬ì— ìˆëŠ”ì§€ í™•ì¸
    if not os.path.exists(os.path.join(model_path, "config.json")):
        # ë¡œì»¬ì— ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ
        model = AutoModelForCausalLM.from_pretrained(
            model_id, device_map={"": 0}, torch_dtype=torch.float16, low_cpu_mem_usage=True
        )
        # ë¡œì»¬ì— ì €ì¥
        print('ëª¨ë¸ ì €ì¥ ì‹œì‘')
        model.save_pretrained(model_path)
    else:
        # ë¡œì»¬ì— ìˆìœ¼ë©´ í•´ë‹¹ ê²½ë¡œì—ì„œ ë¡œë“œ
        print('ëª¨ë¸ ë¡œë“œ ì‹œì‘')
        model = AutoModelForCausalLM.from_pretrained(model_path, device_map={"": 0}, torch_dtype=torch.float16, low_cpu_mem_usage=True)

    model.to(device)
    print('ëª¨ë¸ ë¡œë“œ ì™„ë£Œ')
    
    # í† í¬ë‚˜ì´ì €ê°€ ë¡œì»¬ì— ìˆëŠ”ì§€ í™•ì¸
    if not os.path.exists(os.path.join(tokenizer_path, "tokenizer_config.json")):
        # ë¡œì»¬ì— ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(model_id)
        # ë¡œì»¬ì— ì €ì¥
        tokenizer.save_pretrained(tokenizer_path)
    else:
        # ë¡œì»¬ì— ìˆìœ¼ë©´ í•´ë‹¹ ê²½ë¡œì—ì„œ ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)

    print("Model and Tokenizer loaded")
    # ëª¨ë¸ì„ ì¶”ë¡ ì—ë§Œ ì‚¬ìš©í•  ê²ƒì´ë¯€ë¡œ, 'eval'ëª¨ë“œë¡œ ë³€ê²½
    model.eval()
    
    # ì •ë³´
    #class_index = json.load(open('../path/to/'))

    model.config.use_cache = True
    tokenizer.pad_token = tokenizer.eos_token
    print("ready to infer")
    setting()

#íŒŒì¼ ë¡œë“œ/ ì„ë² ë”©/ ëª¨ë¸ ë¡œë“œ
def setting() :
    load_dotenv() # í† í° ì •ë³´ë¡œë“œ

    ''' #pdf version
    # PDF íŒŒì¼ ë¡œë“œ â‘  ë°ì´í„° ë¡œë“œ
    loader = PyPDFLoader("240208ì½”ê·¸í”Œë ˆì´ë¦¬í”Œë ›êµ­ë¬¸-ì €ìš©ëŸ‰.pdf") #("EYAS_ê°„ì´_ì‚¬ìš©ì„¤ëª…ì„œ.pdf")
    document = loader.load()
    print("ë‚´ìš© ì¼ë¶€ ì¶œë ¥",document[1].page_content[:200])# ë‚´ìš© ì¶”ì¶œ

    docs = []
    docs.extend(loader.load_and_split())
    # â‘¡ ë°ì´í„° ë¶„í• 
    text_splitter = CharacterTextSplitter(
        separator="\n", #ì²­í¬ë¥¼ êµ¬ë¶„í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ë¬¸ìì—´
        chunk_size=1000, # ì²­í¬ì˜ ìµœëŒ€ ê¸¸ì´
        chunk_overlap=200, #ì¸ì ‘í•œ ì²­í¬ ê°„ì— ê²¹ì¹˜ëŠ” ë¬¸ìì˜ ìˆ˜
        length_function=len,#ì²­í¬ì˜ ê¸¸ì´ë¥¼ ê³„ì‚°í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” í•¨ìˆ˜
    )
    texts= text_splitter.split_documents(document)
    
    # â‘¢ ì €ì¥ ë° ê²€ìƒ‰
    # ì„ë² ë”©
    model_name = "hkunlp/instructor-xl"
    model_kwargs={'device':'cpu'}
    encode_kwargs={'normalize_embeddings':True}
    embeddings= HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs= model_kwargs,
        encode_kwargs=encode_kwargs,
    )
    print("Found HuggingFaceEmbeddings!!â™¡ğŸ¤—")
    model_id="mu0gum/AIFT-polyglot-ko-1.3b-ao-instruct-v0.91"

    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id)
    gen = pipeline('text-generation', model=model, tokenizer=tokenizer, max_new_tokens=200)
    
    #ëª¨ë¸(LLM) ì„ ìƒì„±
    llama_llm = HuggingFacePipeline(pipeline=gen)

    # HuggingFacePipeline ê°ì²´ ìƒì„±
    llm = HuggingFacePipeline.from_model_id(
        model_id=model_id, 
        device=0,               # -1: CPU(default), 0ë²ˆ ë¶€í„°ëŠ” CUDA ë””ë°”ì´ìŠ¤ ë²ˆí˜¸ ì§€ì •ì‹œ GPU ì‚¬ìš©í•˜ì—¬ ì¶”ë¡ 
        task="text-generation", # í…ìŠ¤íŠ¸ ìƒì„±
        model_kwargs={"temperature": 0.1, 
                    "max_length": 64},
    )
    print("Found LLM model!!")
    bm25_retriever = BM25Retriever.from_documents(texts)
    bm25_retriever.k = 2

    # ì¸ë±ì‹±: ë¶„í• ëœ í…ìŠ¤íŠ¸ë¥¼ ê²€ìƒ‰ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë§Œë“œëŠ” ë‹¨ê³„// DB ì— ì €ì¥
    chroma_vector = Chroma.from_documents(texts, embeddings)
    chroma_retriever = chroma_vector.as_retriever(search_kwargs={'k':2})
    '''


    ''' #txt version'''
    # txt íŒŒì¼ ë¡œë“œ â‘  ë°ì´í„° ë¡œë“œ
    loader = TextLoader('eyas_docs3.txt')
    data = loader.load()

    # â‘¡ ë°ì´í„° ë¶„í• 
    text_splitter = CharacterTextSplitter(
        separator = '\n',
        chunk_size = 500,
        chunk_overlap  = 100,
        length_function = len,
    )

    texts = text_splitter.split_text(data[0].page_content)
        
    # â‘¢ ì €ì¥ ë° ê²€ìƒ‰
    # ì„ë² ë”©
    model_name = "hkunlp/instructor-xl"
    model_kwargs={'device':'cpu'}
    encode_kwargs={'normalize_embeddings':True}
    # í—ˆê¹…í˜ì´ìŠ¤ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì—¬ ê¸°ë³¸ ì„ë² ë”© ì„¤ì •
    embeddings= HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs= model_kwargs,
        encode_kwargs=encode_kwargs,
    )

    # #ë¡œì»¬ íŒŒì¼ ì €ì¥ì†Œ ì„¤ì •
    # store = LocalFileStore("./cache/")

    # # ìºì‹œë¥¼ ì§€ì›í•˜ëŠ” ì„ë² ë”© ìƒì„±
    # cached_embedder = CacheBackedEmbeddings.from_bytes_store(
    #     embeddings,
    #     store,
    #     namespace=embeddings.model,  # ê¸°ë³¸ ì„ë² ë”©ê³¼ ì €ì¥ì†Œë¥¼ ì‚¬ìš©í•˜ì—¬ ìºì‹œ ì§€ì› ì„ë² ë”©ì„ ìƒì„±
    # )

    # # storeì—ì„œ í‚¤ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    # list(store.yield_keys())




    def cache_embed_wrapper(embedding_model, local_store_path=None):
        if local_store_path is not None:
            store = LocalFileStore(local_store_path)
        else:
            store = InMemoryByteStore()

        cache_embed_wrapper = CacheBackedEmbeddings.from_bytes_store(embedding_model,
                                                                    document_embedding_cache=store,
                                                                    namespace=embedding_model.model_name)
        return cache_embed_wrapper

    print("Found HuggingFaceEmbeddings!!â™¡ğŸ¤—")

    def search(query: str, k: int = 3 ):
        """a function that embeds a new query and returns the most probable results"""
        embedded_query = ST.encode(query) # embed new query
        scores, retrieved_examples = data.get_nearest_examples( # retrieve results
            "embeddings", embedded_query, # compare our new embedded query with the dataset embeddings
            k=k # get only top k results
        )
        return scores, retrieved_examples


    #model_id="momo/polyglot-ko-12.8b-Chat-QLoRA-Merge"#"gwonny/llama-2-koen-13b-QLoRA-NEFTune-kolon-v0.1"
    #"mu0gum/AIFT-polyglot-ko-1.3b-ao-instruct-v0.91"#"nlpai-lab/kullm-polyglot-5.8b-v2"#"Changgil/K2S3-Llama2-13b-v1.0"#"beomi/open-llama-2-ko-7b"#"KRAFTON/KORani-v2-13B"
    #"beomi/open-llama-2-ko-7b"#"beomi/OPEN-SOLAR-KO-10.7B"#"cocoirun/AIFT-42dot-PLM-1.3B-ao-instruct-all-v0.4-ff-e1"#"mu0gum/AIFT-polyglot-ko-1.3b-ao-instruct-v0.91"

    # tokenizer = AutoTokenizer.from_pretrained(model_id)
    # model = AutoModelForCausalLM.from_pretrained(model_id)

    # use quantization to lower GPU usage
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.bfloat16
    )

    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        quantization_config=bnb_config,
        resume_download = True
    )
    terminators = [
        tokenizer.eos_token_id,
        tokenizer.convert_tokens_to_ids("<|eot_id|>")
    ]

    SYS_PROMPT = """You are an assistant for answering questions.
    You are given the extracted parts of a long document and a question. Provide a conversational answer.
    If you don't know the answer, just say "I do not know." Don't make up an answer."""

    def format_prompt(prompt,retrieved_documents,k):
        """using the retrieved documents we will prompt the model to generate our responses"""
        PROMPT = f"Question:{prompt}\nContext:"
        for idx in range(k) :
            PROMPT+= f"{retrieved_documents['text'][idx]}\n"
        return PROMPT

    def generate(formatted_prompt):
        formatted_prompt = formatted_prompt[:2000] # to avoid GPU OOM
        messages = [{"role":"system","content":SYS_PROMPT},{"role":"user","content":formatted_prompt}]
        # tell the model to generate
        input_ids = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_tensors="pt"
        ).to(model.device)
        outputs = model.generate(
            input_ids,
            max_new_tokens=1024,
            eos_token_id=terminators,
            do_sample=True,
            temperature=0.6,
            top_p=0.9,
        )
        response = outputs[0][input_ids.shape[-1]:]
        return tokenizer.decode(response, skip_special_tokens=True)

    def rag_chatbot(prompt:str,k:int=2):
        scores , retrieved_documents = search(prompt, k)
        formatted_prompt = format_prompt(prompt,retrieved_documents,k)
        return generate(formatted_prompt)

    rag_chatbot("what's anarchy ?", k = 2)
    print("ğŸ«ğŸ«ğŸ«")
    print(rag_chatbot("what's anarchy ?", k = 2))
    gen = pipeline('text-generation', model=model, tokenizer=tokenizer, max_new_tokens=200)

    #print("í¬ê¸° í™•ì¸ìš©ğŸ‘§ğŸ» ", model.num_parameters())
    #ëª¨ë¸(LLM) ì„ ìƒì„± - ëª¨ë¸/íŒŒì´í”„ë¼ì¸ì„ ìˆ˜ë™ìœ¼ë¡œ ì „ë‹¬
    global llama_llm
    llama_llm = HuggingFacePipeline(pipeline=gen)

    #global gpu_llm
    # HuggingFacePipeline ê°ì²´ ìƒì„± - from_model_idë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ë¥¼ ì§€ì •í•¨ìœ¼ë¡œì¨ ë¡œë“œí•  ìˆ˜ ìˆìŒ
    # llm = HuggingFacePipeline.from_model_id(
    #     model_id=model_id, 
    #     device=0,               # -1: CPU(default), 0ë²ˆ ë¶€í„°ëŠ” CUDA ë””ë°”ì´ìŠ¤ ë²ˆí˜¸ ì§€ì •ì‹œ GPU ì‚¬ìš©í•˜ì—¬ ì¶”ë¡ 
    #     task="text-generation", # í…ìŠ¤íŠ¸ ìƒì„±
    #     model_kwargs={"temperature": 0.1, 
    #                 "max_length": 64},
    # )
    # gpu_llm = HuggingFacePipeline.from_model_id(
    #     model_id=model_id,  # ì‚¬ìš©í•  ëª¨ë¸ì˜ IDë¥¼ ì§€ì •í•©ë‹ˆë‹¤.
    #     task="text-generation",  # ìˆ˜í–‰í•  ì‘ì—…ì„ ì„¤ì •í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” í…ìŠ¤íŠ¸ ìƒì„±ì…ë‹ˆë‹¤.
    #     # ì‚¬ìš©í•  GPU ë””ë°”ì´ìŠ¤ ë²ˆí˜¸ë¥¼ ì§€ì •í•©ë‹ˆë‹¤. "auto"ë¡œ ì„¤ì •í•˜ë©´ accelerate ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    #     device=0,
    #     # íŒŒì´í”„ë¼ì¸ì— ì „ë‹¬í•  ì¶”ê°€ ì¸ìë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜ë¥¼ 10ìœ¼ë¡œ ì œí•œí•©ë‹ˆë‹¤.
    #     pipeline_kwargs={"max_new_tokens": 64},
    # )
    # gpu_llm = HuggingFacePipeline.from_model_id(
    #     model_id=model_id,  # ì‚¬ìš©í•  ëª¨ë¸ì˜ IDë¥¼ ì§€ì •í•©ë‹ˆë‹¤.
    #     task="text-generation",  # ìˆ˜í–‰í•  ì‘ì—…ì„ ì„¤ì •í•©ë‹ˆë‹¤.
    #     device=0,  # GPU ë””ë°”ì´ìŠ¤ ë²ˆí˜¸ë¥¼ ì§€ì •í•©ë‹ˆë‹¤. -1ì€ CPUë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
    #     batch_size=2,  # ë°°ì¹˜ í¬ê¸°së¥¼ ì¡°ì •í•©ë‹ˆë‹¤. GPU ë©”ëª¨ë¦¬ì™€ ëª¨ë¸ í¬ê¸°ì— ë”°ë¼ ì ì ˆíˆ ì„¤ì •í•©ë‹ˆë‹¤.
    #     model_kwargs={
    #         "temperature": 0,
    #         "max_length": 64,
    #     },  # ëª¨ë¸ì— ì „ë‹¬í•  ì¶”ê°€ ì¸ìë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
    # )


    template = """Answer the following question in Korean.
    #Question: 
    {question}

    #Answer: """  # ì§ˆë¬¸ê³¼ ë‹µë³€ í˜•ì‹ì„ ì •ì˜í•˜ëŠ” í…œí”Œë¦¿
    prompt = PromptTemplate.from_template(template)  # í…œí”Œë¦¿ì„ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ê°ì²´ ìƒì„±
    
    # # í”„ë¡¬í”„íŠ¸ì™€ ì–¸ì–´ ëª¨ë¸ì„ ì—°ê²°í•˜ì—¬ ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    # gpu_chain = prompt | llama_llm.bind(stop=["\n\n"])

    # questions = []
    # for i in range(4):
    #     # ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    #     questions.append({"question": f"ìˆ«ì {i} ì´ í•œê¸€ë¡œ ë­ì—ìš”?"})

    # answers = gpu_chain.batch(questions)  # ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ ì²˜ë¦¬í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    # for answer in answers:
    #     print(answer)  # ìƒì„±ëœ ë‹µë³€ì„ ì¶œë ¥í•©ë‹ˆë‹¤.

    print("Found LLM model!!")

    # llm_chain = LLMChain(prompt=prompt, llm=llama_llm)
    # question = "ì–´ê¹¨ ì•„íŒŒ. ì–´ë–¡í•´?"
    # print(question)
    # print(llm_chain.run(question=question))
    

    global bm25_retriever
    #ìœ ì‚¬ë„ ê²€ìƒ‰
    bm25_retriever = BM25Retriever.from_texts(texts)
    bm25_retriever.k = 2

    # ì¸ë±ì‹±: ë¶„í• ëœ í…ìŠ¤íŠ¸ë¥¼ ê²€ìƒ‰ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë§Œë“œëŠ” ë‹¨ê³„// DB ì— ì €ì¥
    #í‚¤ì›Œë“œ ê²€ìƒ‰
    global chroma_retriever
    #ì›ë³¸
    # chroma_vector = Chroma.from_texts(texts, embeddings)
    # chroma_retriever = chroma_vector.as_retriever(search_kwargs={'k':2})
    #ì›ë³¸

    start = time.time()
    chroma_vector = Chroma.from_texts(texts, cache_embed_wrapper(embeddings))
    chroma_retriever = chroma_vector.as_retriever(search_kwargs={'k':2})
    math.factorial(100000)
    end = time.time()

    print(f"{end - start:.5f} sec")
    #ì¶œì²˜: https://blockdmask.tistory.com/549 [ê°œë°œì ì§€ë§ìƒ:í‹°ìŠ¤í† ë¦¬]

    # ì½”ë“œ ì‹¤í–‰ ì‹œê°„ì„ ì¸¡ì •í•©ë‹ˆë‹¤.
    
    global ensemble_retriever
    ensemble_retriever = EnsembleRetriever(
        retrievers =[bm25_retriever, chroma_retriever], weights = [0.7, 0.3]
    )

    global memory
    memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)
    global conversation_chain
    conversation_chain = ConversationalRetrievalChain.from_llm(
        llm=llama_llm,
        retriever=ensemble_retriever,
        memory=memory
    )
    # ì£¼ì„ 2024.04.15
    # ensemble_docs = ensemble_retriever.get_relevant_documents("what is some investing advice?")
    # len(ensemble_docs)

    # prompt_template = """ë§ˆì§€ë§‰ ì§ˆë¬¸ì— ë‹µí•˜ë ¤ë©´ ë‹¤ìŒ ë¬¸ë§¥ì„ ì‚¬ìš©í•˜ì„¸ìš”.
    # ë‹µì„ ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ê³  ë‹µì„ ë§Œë“¤ì–´ë‚´ë ¤ê³  í•˜ì§€ ë§ˆì„¸ìš”.

    # {context}

    # Question: {question}
    # """
    # PROMPT = PromptTemplate(
    #     template=prompt_template, input_variables=["context", "question"]
    # )

    # question = "ë‹¬ë ¤ë¼ ë‹¤ëŒì´ëŠ” ë­ì•¼?"

    # def format_docs(docs):
    #     # ê²€ìƒ‰í•œ ë¬¸ì„œ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ë¬¸ë‹¨ìœ¼ë¡œ í•©ì³ì¤ë‹ˆë‹¤.
    #     return "\n\n".join(doc.page_content for doc in docs)
    # chain =(
    #     {"context": ensemble_retriever | format_docs, "question": RunnablePassthrough(),}
    #     | PROMPT
    #     | llama_llm
    # )
    # result = chain.invoke("ë‹¬ë ¤ë¼ ë‹¤ëŒì´ëŠ” ë­ì•¼?")
    # print(result)
    # ì£¼ì„ 2024.04.15

    # print(llama_llm.predict(text=PROMPT.format_prompt(
    #     context=ensemble_docs,
    #     question=question
    # ).text))

    # prompt_template = """ë§ˆì§€ë§‰ ì§ˆë¬¸ì— ë‹µí•˜ë ¤ë©´ ë‹¤ìŒ ë¬¸ë§¥ì„ ì‚¬ìš©í•˜ì„¸ìš”.
    # ë‹µì„ ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ê³  ë‹µì„ ë§Œë“¤ì–´ë‚´ë ¤ê³  í•˜ì§€ ë§ˆì„¸ìš”.

    # {context}

    # Question: {question}
    # Answer:"""
    # PROMPT = PromptTemplate(
    #     template=prompt_template, input_variables=["context", "question"]
    # )

    # question = "ë‹¬ë ¤ë¼ ë‹¤ëŒì´ëŠ” ë­ì•¼?"

    # print(llama_llm.predict(text=PROMPT.format_prompt(
    #     context=ensemble_docs,
    #     question=question
    # ).text))


    # ë‹¤ì¤‘ ì¿¼ë¦¬ ê²€ìƒ‰ê¸°
    # vectordb = FAISS.from_texts(texts=texts, embedding=embeddings)
    # global retriever_from_llm
    # retriever_from_llm = MultiQueryRetriever.from_llm(retriever=vectordb.as_retriever(),
    #                                                 llm=llama_llm)

def handle_userinput(user_question):
    # response = st.session_state.conversation({'question': user_question})
    # st.session_state.chat_history = response['chat_history']
    global rp, chat_history, helpfulAnswer_n
    rp = conversation_chain({'question': user_question})
    chat_history = rp['chat_history']

    global text
    for i, message in enumerate(chat_history):
        if i % 2 == 0:
            #print(i," wonderful")
            print(i,"//user: " + message.content)
        else:
            print(i,"//bot: " + message.content)
            #text = message.content
            helpfulAnswer = message.content
            # s = user_question+"\nHelpful Answer:"
            # if(s in helpfulAnswer):
            #     print("ê²€ìƒ‰ ë¬¸ìì—´ì´ ë©”ì¸ ë¬¸ìì—´ì— í¬í•¨ë˜ì–´ ìˆìŒğŸ˜›")
            helpfulAnswer_n = helpfulAnswer.split(user_question+"\nHelpful Answer:")
          
            

def sendQuestion(question) :
    def format_docs(docs):
        # ê²€ìƒ‰í•œ ë¬¸ì„œ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ë¬¸ë‹¨ìœ¼ë¡œ í•©ì³ì¤ë‹ˆë‹¤.
        return "\n\n".join(doc.page_content for doc in docs)

    # í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    prompt = hub.pull("rlm/rag-prompt")

    #SQLite Cache 20240416
    set_llm_cache(SQLiteCache(database_path="my_llm_cache.db"))


    handle_userinput(question)

    print("í…ŒìŠ¤íŠ¸" + time.strftime('%Y.%m.%d - %H:%M:%S'))

    # ì§€ê¸ˆ í…ŒìŠ¤íŠ¸ í•˜ëŠ” ê±°ëŠ” ì—¬ê¸° rag_chain ì•ˆ ë§ë‹¤
    """
    #ì²´ì¸ ìƒì„±(Create Chain)
    rag_chain = (
        {"context": ensemble_retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llama_llm
        | StrOutputParser()
    )


    print("\ní˜„ì¬ì‹œê°„" + time.strftime('%Y.%m.%d - %H:%M:%S'))

    # now = time.localtime()
    # print ("ì²´ì¸ ìƒì„± ì‹œê°„:","%04d/%02d/%02d %02d:%02d:%02d" % (now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec))
    global answer
    response = rag_chain.invoke(question+" ë‘ë¬¸ì¥ìœ¼ë¡œ ë§í•´ì¤˜. ì¸ë±ìŠ¤ ë¹¼ê³  ë‹µí•´ì¤˜")
    #print("Answer:" in response) True
    #print("Answer:\n" in response)
    if "Answer:" in response:
        answer = response.split("Answer:", 1)[1]
        #answer.replace("\n", " ")
        if "ë‹µë³€:\n" in response:
            answer = response.split("ë‹µë³€:", 1)[1]
        print("ì´ê²ƒì´ answerë‹¤ : ", answer)
    # now = time.localtime()
    # print ("ë‹µë³€ ìƒì„±ì‹œê°„:","%04d/%02d/%02d %02d:%02d:%02d" % (now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec))
    print(f"ğŸ¤–[AI]\n{response}", end='') #, end='' ì¤„ë°”ê¿ˆì—†ëŠ” print
    #text = gen(input("ì§ˆë¬¸ ë‚´ìš© ì…ë ¥: "), model=model, tokenizer=tokenizer, device=device)
    #text = LLM_infer(input("ì§ˆë¬¸ ë‚´ìš© ì…ë ¥: "))
    # textì—ì„œ </s>ë¥¼ ì‚­ì œ
    textArr = response.split(".")
    #print(f"ë°°ì—´ ê¸¸ì´ {len(textArr)}")
    if(len(textArr) > 1):
        for i in range(0, len(textArr)):
            for j in range(i+1, len(textArr)):
                if(j < len(textArr)):
                    if(textArr[i] == textArr[j]):
                        del textArr[j]
    print(f"\ní…ìŠ¤íŠ¸ ì„ë‹¹ {response}")
    #text = ".".join(textArr)
    if(len(textArr) > 2):
        text = textArr[0] +"."+ textArr[1] +"."
    else:
        text = ".".join(textArr)
    print(f"==========================ì¤‘ë³µ ì œê±° í™•ì¸=================\n {text}")
    text2 = text.replace('\n', ' ')
    text = text2.replace('"answer": "', "")
    text = answer
    """

    '''
    # ì•™ìƒë¸” ê²°ê³¼ ë³´ê¸°
    def pretty_print(docs):
        for i, doc in enumerate(docs):
            print(f"[{i+1}] {doc.page_content}")

    sample_query = question
    print(f"[Query]\n{sample_query}\n")
    relevant_docs = bm25_retriever.get_relevant_documents(sample_query)
    print("[BM25 Retriever]")
    pretty_print(relevant_docs)
    print("===" * 20)
    relevant_docs = chroma_retriever.get_relevant_documents(sample_query)
    print("[Chroma Retriever]")
    pretty_print(relevant_docs)
    print("===" * 20)
    relevant_docs = ensemble_retriever.get_relevant_documents(sample_query)
    print("[Ensemble Retriever]")
    pretty_print(relevant_docs)
    memory = ConversationBufferMemory(
        memory_key='chat_history', return_messages=True, output_key='answer')
    conversation_chain = ConversationalRetrievalChain.from_llm(
        llm=llama_llm,
        retriever=ensemble_retriever,#retriever=vectorstore.as_retriever(),#retriever=ensemble_retriever,# retriever ê°€ì ¸ì˜´
        memory=memory
    )

    chat_history = []
    #result = {}
    query = "ìì¡´ê°ì€ ë­ì•¼?"#"EYASë¥¼ ì–¼ë§ˆë‚˜ í•´ì•¼ ì¸ì§€ëŠ¥ë ¥ì´ ì¢‹ì•„ì§€ë‚˜ìš”?"

    result = conversation_chain.invoke({"question": query, "chat_history": chat_history}, return_only_outputs=True)
    print(query)
    print(result["answer"])

    llm_chain = LLMChain(prompt=prompt, llm=llama_llm)
    question = "ìì‹ ê°ì´ ë­ì•¼?"
    print(question)
    print(llm_chain.run(question=question))
    '''
    # now = time.localtime()
    # print ("ì¤‘ë³µì œê±° í›„ ì‹œê°„:","%04d/%02d/%02d %02d:%02d:%02d" % (now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec))

    #ì²´ì¸ ìƒì„±(Create Chain)
    # rag_chain2 = (
    #     {"context": retriever_from_llm | format_docs, "question": RunnablePassthrough()}
    #     | prompt
    #     | llama_llm
    #     | StrOutputParser()
    # )
    # response = rag_chain2.invoke(question)
    # print(f"[ë‹¤ì¤‘ ì‘ë‹µ ë¦¬íŠ¸ë¦¬ë²„ ë‹µë³€]\n{response}")

    finalAns = helpfulAnswer_n[-1]
    print("finalAns ==> ",finalAns)
    if("\n" in finalAns):
        #finalAns = ''.join(helpfulAnswer_n[-1].splitlines()).split() #ë¬¸ì¥ì˜ ê³µë°±ë„ ëŠìŒ # ë¦¬ìŠ¤íŠ¸ì—ì„œ ë¹ˆ ë¬¸ìì—´ì¸ ì›ì†Œ ì œê±°
        #finalAns= list(filter(None, helpfulAnswer_n[-1].splitlines())) ë°°ì—´ë¡œ ë§Œë“¤ì–´ì ¸ì„œ string í›„ì²˜ë¦¬ ì¢€
        print("ì˜ ì •ë¦¬ëëŠ”ì§€ í™•ì¸ " ,finalAns )
        
        #finalAns = finalAns[0] #U
        finalAns=findNicdAnswer(finalAns, question)
        print("ì˜ ì •ë¦¬ëëŠ”ì§€ í™•ì¸ " ,finalAns )
        #finalAns = helpfulAnswer_n[-1].splitlines()[0]
        # ansList = helpfulAnswer_n[-1].splitlines()
        # for i in ansList:
        #     if(ansList[i]==""):
        #         continue
        #     else
        #         finalAns = ansList[i]
        #         break

        # if(len(finalAns) > 10):
        #     finalAns = ' '.join(finalAns)
        #     print(len(helpfulAnswer_n[-1].splitlines()))
        #     finalAns = findNicdAnswer(finalAns, question)
        #     #finalAns = ansList[3]
        #     #helpfulAnswer_2 = finalAns.split("Helpful Answer:")[-1]
        #     print("í™•ì¸ ğŸ™†ğŸ»â€â™€ï¸", helpfulAnswer_2[-1])
        print("ğŸ˜ë“¤ì–´ì™”ìœ¼\n"+str(helpfulAnswer_n[-1].splitlines()))
    #print("í™•ì¸ ğŸ™†ğŸ»â€â™€ï¸", helpfulAnswer_n[-1])
    return finalAns

def findNicdAnswer(sentence, user_question):
    s = user_question+"\nHelpful Answer:"
    print(s in sentence)
    if(s in sentence):
    #     print("ê²€ìƒ‰ ë¬¸ìì—´ì´ ë©”ì¸ ë¬¸ìì—´ì— í¬í•¨ë˜ì–´ ìˆìŒğŸ˜›")
        niceAnswer = sentence.split(user_question+"\nHelpful Answer:\n")[-1]
    else:
        niceAnswer = sentence
    print("ğŸ¥¨niceAnswer: ",niceAnswer)
    ansList = niceAnswer.splitlines()
    # for ans in ansList:
    #     if(ans==""):
    #         continue
    #     else:
    #         finalAns = ans
    #         break
    finalAns = ansList[-1]
    return finalAns
          
def main():
    settingModel()
	
    


if __name__ == "__main__":
    main()